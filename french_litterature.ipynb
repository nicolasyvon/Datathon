{"cells":[{"cell_type":"markdown","source":["## Introduction"],"metadata":{"id":"Qe5cqtwsvzPC"}},{"cell_type":"markdown","source":["In a world where the negative seems to come out more easily than the positive, one wonders if this has an impact on the population. For this we can use the literature as a barometer, as it gives us an idea of what emotion is most prevalent in the population.\n","We wonder which lexicon is the most used in french litterature, the love or the hate one ?"],"metadata":{"id":"76XoNagOv2Sx"}},{"cell_type":"markdown","source":["# Dataprocessing"],"metadata":{"id":"akgGkXg3wu4l"}},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"fSCs4zTAw8Ba"}},{"cell_type":"code","source":["# Set to True if working in Amazon SageMaker\n","SAGEMAKER = False"],"metadata":{"id":"szGR_R4Nw9z7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","import os\n","\n","APPS_HOME      = os.getcwd() + \"/apps\"\n","\n","SPARK_VERSION  = \"3.0.0\"\n","HADOOP_VERSION = \"2.7\"\n","AUT_VERSION    = \"0.91.0\"\n","JAVA_VERSION   = \"11\"\n","\n","SPARK_HADOOP_VERSION = \"spark-{}-bin-hadoop{}\".format(SPARK_VERSION, HADOOP_VERSION)\n","\n","if SAGEMAKER:\n","    !sudo amazon-linux-extras install java-openjdk11 -y\n","    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-11-openjdk-11.0.16.0.8-1.amzn2.0.1.x86_64\"\n","else:\n","    !apt-get install openjdk-\"$JAVA_VERSION\"-jdk-headless\n","    os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-{}-openjdk-amd64\".format(JAVA_VERSION)\n","\n","!pip install -q findspark\n","\n","!wget https://archive.apache.org/dist/spark/spark-\"$SPARK_VERSION\"/\"$SPARK_HADOOP_VERSION\".tgz\n","!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\".zip\n","!wget https://github.com/archivesunleashed/aut/releases/download/aut-\"$AUT_VERSION\"/aut-\"$AUT_VERSION\"-fatjar.jar\n","\n","!tar -xf \"$SPARK_HADOOP_VERSION\".tgz\n","!mkdir -p \"$APPS_HOME\"\n","!mv spark-* aut-* \"$APPS_HOME\"\n","\n","!rm -rf sample_data \"$APPS_HOME\"/\"$SPARK_HADOOP_VERSION\".tgz\n"],"metadata":{"id":"ywJjxj93xBTN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spark init"],"metadata":{"id":"rsb537sMxEB2"}},{"cell_type":"code","source":["import os\n","import findspark\n","\n","SPARK_DRIVER_MEMORY   = \"8g\"\n","\n","os.environ[\"SPARK_HOME\"] = \"{}/{}\".format(APPS_HOME, SPARK_HADOOP_VERSION)   \n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-memory {0} --jars {2}/aut-{1}-fatjar.jar --py-files {2}/aut-{1}.zip pyspark-shell'.format(SPARK_DRIVER_MEMORY, AUT_VERSION, APPS_HOME)\n","\n","findspark.init()"],"metadata":{"id":"hb_hE5elxGaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import SQLContext\n","from pyspark.sql.functions import desc, col, udf\n","from pyspark.sql.types import StringType\n","\n","sc = pyspark.SparkContext.getOrCreate()\n","sqlContext = SQLContext(sc)\n","\n","sc"],"metadata":{"id":"M5ja7a7oxLt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading datasets"],"metadata":{"id":"vyeSajYBxWK2"}},{"cell_type":"code","source":["%%capture \n","!pip install -q gdown\n","\n","# !gdown https://drive.google.com/drive/folders/1xqDsY5KOeK5OMhW39EH37l79Pn-v59B_?usp=sharing -O ./LIFRANUM/autre --folder\n","# !gdown https://drive.google.com/drive/folders/170j3r23YJBlOpGsKrcZRSs3bqrS03qhi?usp=sharing -O ./LIFRANUM/cartoweb --folder\n","# !gdown https://drive.google.com/drive/folders/1NLuWLOldfmpwPeAr9Th_HCeH6ZoSw0zr?usp=sharing -O ./LIFRANUM/lifranum-method --folder\n","!gdown https://drive.google.com/drive/folders/1wehg3nnCks9iVIvuXMZ5u685ocq__dQe?usp=sharing -O ./LIFRANUM/repo-ecritures-num --folder"],"metadata":{"id":"dBd1VUKDxbR8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating dataframe"],"metadata":{"id":"TH9hav8jxlWO"}},{"cell_type":"code","source":["!pip install tldextract"],"metadata":{"id":"VDDssUCixqQG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from aut import *\n","import tldextract\n","WARCs_path = \"LIFRANUM/repo-ecritures-num/*.warc*\""],"metadata":{"id":"5tsSVWkOxou0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from urllib.parse import urlparse\n","\n","@udf(\"string\")\n","def extract_hostname(s):\n","    return urlparse(s).hostname\n","\n","@udf(\"string\")\n","def extract_url_domain(s):\n","  return tldextract.extract(s).domain\n","\n","@udf(\"string\")\n","def extract_url_subdomain(s):\n","  return tldextract.extract(s).subdomain\n","\n","@udf(\"string\")\n","def extract_url_tld(s):\n","  return tldextract.extract(s).suffix\n","\n","@udf(\"string\")\n","def extract_url_registered_domain(s):\n","  return tldextract.extract(s).registered_domain\n","\n","@udf(\"string\")\n","def extract_url_domain_reversed(s):\n","  text=urlparse(s).hostname\n","  t=text.split('.')\n","  t.reverse()\n","  text='.'.join(t)\n","  return text\n","\n","@udf(\"string\")\n","def extract_port(s):\n","  return urlparse(s).port\n","\n","@udf(\"string\")\n","def extract_requete(s):\n","  return urlparse(s).scheme\n","\n","df2 = WebArchive(sc, sqlContext, WARCs_path).webpages()\\\n","  .withColumn(\"hostname\", extract_hostname(\"url\") )\\\n","  .withColumn(\"url_domain\", extract_url_domain(\"url\"))\\\n","  .withColumn(\"url_subdomain\", extract_url_subdomain(\"url\"))\\\n","  .withColumn(\"url_tld\", extract_url_tld(\"url\"))\\\n","  .withColumn(\"url_domain_reversed\", extract_url_domain_reversed(\"url\"))\\\n","  .withColumn(\"url_registered_domain\", extract_url_registered_domain(\"url\"))\\\n","  .withColumn(\"port\", extract_port(\"url\"))\\\n","  .withColumn(\"requete\", extract_requete(\"url\"))\n","df2.show(1, True);"],"metadata":{"id":"twCNjqnUxvdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data formatting"],"metadata":{"id":"e3vq6apSx0Ku"}},{"cell_type":"code","source":["from aut import remove_html, remove_http_header\n","import re\n","\n","def count_words(res, wordsLists):\n","  charToReplace = {\n","      \"'\": \" \",\n","      \",\": \" \",\n","      \"/\": \" \",\n","      \".\": \" \",\n","      \":\": \" \",\n","      \";\": \" \",\n","      \"!\": \" \",\n","      \"?\": \" \",\n","      \"\\\"\": \" \",\n","      \"é\": \"e\",\n","      \"è\": \"e\",\n","      \"ê\": \"e\",\n","      \"-\": \" \",\n","      \"à\": \"a\",\n","  }\n","\n","  # building dictionnary\n","  ws_feeling = {}\n","  lists_counts = {}\n","\n","  for i in res:\n","    # cleaning data\n","    domain = i[0].lower()\n","    cleanedData = i[1].lower()\n","    for char in charToReplace:\n","      cleanedData = cleanedData.replace(char, charToReplace[char])\n","    ttlRowLen = len(cleanedData.split())\n","    # dictionary domain init\n","    if domain in ws_feeling:\n","      ws_feeling[domain][\"tot_word\"] = ws_feeling[domain][\"tot_word\"] + ttlRowLen\n","    else:\n","      ws_feeling[domain] = {\"tot_word\": ttlRowLen}\n","    \n","    # counting words for all words lists\n","    for i, aList in enumerate(wordsLists):\n","      index_str = str(i)\n","\n","      for word in aList:\n","        wordCount = len(re.findall(r\"(?i)\\ \"+ word + r\"\\b\", cleanedData))\n","\n","        if wordCount > 0:\n","          #creating dictionary in lists_counts if doesn't exists\n","          if index_str not in lists_counts:\n","            lists_counts[index_str] = {}\n","          # adding count to count storage for this list\n","          if word in lists_counts[index_str]:\n","            lists_counts[index_str][word] = lists_counts[index_str][word] + wordCount\n","          else:\n","            lists_counts[index_str][word] = wordCount\n","          # adding count to website list\n","          if domain in ws_feeling:\n","            if index_str in ws_feeling[domain]:\n","              ws_feeling[domain][index_str] = ws_feeling[domain][index_str] + wordCount\n","            else:\n","              ws_feeling[domain][index_str] = wordCount\n","          else:\n","            ws_feeling[domain] = {index_str: wordCount}\n","  \n","  # adding 0 entries keys to dictionary\n","  nbOfWordsLists = len(wordsLists)\n","  for domain in ws_feeling:\n","    for i in range(nbOfWordsLists):\n","      if str(i) not in ws_feeling[domain]:\n","        ws_feeling[domain][str(i)] = 0\n","\n","  return (ws_feeling, lists_counts)"],"metadata":{"id":"11Clka5vx5ao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fetching data from df\n","\n","res = df2\\\n",".withColumn('text', remove_html(remove_http_header('content')))\\\n",".select(['url_domain', 'text'])\\\n",".take(5000)"],"metadata":{"id":"V8jxIdUsyBdO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fetching love & hate lexical fields\n","\n","!gdown https://drive.google.com/drive/folders/1M2-DSdt641PMtG1yLGVDERszuigh2h_t?usp=share_link -O ./champsLexical --folder\n","\n","fileRead=open(\"./champsLexical/AmourTmp.txt\")\n","texte=fileRead.read()\n","fileRead.close()\n","\n","loveList=sorted(texte.split('\\n'))\n","n=len(loveList)\n","for k in range(0,n):\n","  loveList[k]=loveList[k][:-1]\n","\n","fileRead=open(\"./champsLexical/HaineTmp.txt\")\n","texte=fileRead.read()\n","fileRead.close()\n","\n","hateList=sorted(texte.split('\\n'))\n","n=len(hateList)\n","for k in range(0,n):\n","  hateList[k]=hateList[k][:-1]"],"metadata":{"id":"zPyjXQjZyPxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count_words() inputs\n","wordsLists = [loveList, hateList] # loveList = 0, hateList = 1"],"metadata":{"id":"kJY_RiZ9ynNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dic_website_feeling, lists_counts = count_words(res, wordsLists)\n","dic_nb_love_word = lists_counts['0']\n","dic_nb_hate_word = lists_counts['1']"],"metadata":{"id":"vALdQtQDyvzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dic_website_feeling,\"\\n\",dic_nb_love_word)"],"metadata":{"id":"1l1JDGzbzLSf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B10EjLp2BADr"},"source":["# Datavisualisation"]},{"cell_type":"markdown","metadata":{"id":"IKnJ1y-mBADx"},"source":["## Import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSbW5sc4BAD1"},"outputs":[],"source":["!pip install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOWIx32_BAD8"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"BVFdb8lVBAD_"},"source":["## Data manipulation"]},{"cell_type":"markdown","source":["Exemple de dictionnaires (penser a mettre les vraies informations)"],"metadata":{"id":"M2vH0nfGGuQO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"doE_yGS8BAEB"},"outputs":[],"source":["#Ce dictionnaire contient pour chaque site le nombre d'occurence de mots du champ lexical amour et haine\n","#dic_website_feeling = {\"www.google.com\" : {\"love\" : 90, \"hate\" : 7, \"tot_word\" : 150},\"www.facebook.com\" : {\"love\" : 10, \"hate\" : 22, \"tot_word\" : 568},\"www.facebak.com\" : {\"love\" : 13, \"hate\" : 12, \"tot_word\" : 243}}\n","\n","#Ces deux dictionnaires doivent contenir le top 5 des mots les plus représentés de leur champ lexical respectif. (/!\\ Ne pas mettre tous les mots sinon le graphe ne sera pas lisible /!\\)\n","#dic_nb_love_word = {\"heart\" : 19, \"happy\" : 2, \"joy\" : 31}\n","#dic_nb_hate_word = {\"kill\" : 13, \"murder\" : 8, \"hate\" : 54}"]},{"cell_type":"code","source":["#Création du dictionnaire pour le troisième graphe\n","\n","top_occurence_website = {\n","    'website' : [],\n","    'occurence' : [],\n","    'color' : []\n","}\n","top_occurence_bubble = {\n","    'website' : [],\n","    'occurence' : [],\n","    'color' : []\n","}\n","\n","for i in dic_website_feeling:\n","    top_occurence_website['website'].append(i)\n","    top_occurence_website['occurence'].append((dic_website_feeling[i][\"0\"] + dic_website_feeling[i][\"1\"])/dic_website_feeling[i][\"tot_word\"])\n","    top_occurence_website[\"color\"].append('#%02x%02x%02x' % (int(255*dic_website_feeling[i][\"0\"] / (1 + (dic_website_feeling[i][\"0\"] + dic_website_feeling[i][\"1\"]))),130,130))\n","    if ((dic_website_feeling[i][\"0\"] + dic_website_feeling[i][\"1\"])==0):\n","      continue\n","    top_occurence_bubble['website'].append(i)\n","    top_occurence_bubble['occurence'].append((dic_website_feeling[i][\"0\"] + dic_website_feeling[i][\"1\"])/dic_website_feeling[i][\"tot_word\"])\n","    top_occurence_bubble[\"color\"].append('#%02x%02x%02x' % (int(255*dic_website_feeling[i][\"0\"] / (1 + (dic_website_feeling[i][\"0\"] + dic_website_feeling[i][\"1\"]))),130,130))"],"metadata":{"id":"KKVmnOJJMzVP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxXrv_08BAED"},"outputs":[],"source":["def normalizer_hate_love(dic):\n","    normalized_dic = {}\n","    for i in dic:\n","        if (dic[i][\"0\"] + dic[i][\"1\"]) == 0:\n","            normalized_dic[i] = (0.5,0)\n","        else:\n","            normalized_dic[i] = (dic[i][\"0\"] / (dic[i][\"0\"] + dic[i][\"1\"]), (dic[i][\"0\"] + dic[i][\"1\"])) \n","    return normalized_dic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUzPbrxkBAEH"},"outputs":[],"source":["#Création du dictionnaire pour le premier graphe\n","\n","dic_norm = normalizer_hate_love(dic_website_feeling)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oelZUpi0BAEK"},"outputs":[],"source":["#Création des listes pour réaliser le deuxième graphe\n","\n","y_nhl = [dic_norm[i][0] for i in dic_norm]\n","x_nhl = [dic_norm[i][1] for i in dic_norm]\n","\n","#h_nl = [dic_nb_love_word[i] for i in dic_nb_love_word]\n","#name_l = [i for i in dic_nb_love_word]\n","#h_nh = [dic_nb_hate_word[i] for i in dic_nb_hate_word]\n","#name_h = [i for i in dic_nb_hate_word]\n","\n","sorted_love = sorted(dic_nb_love_word, key=dic_nb_love_word.get, reverse=True)[:5]\n","sorted_love_values = [dic_nb_love_word[i] for i in sorted_love]\n","sorted_hate = sorted(dic_nb_hate_word, key=dic_nb_hate_word.get, reverse=True)[:5]\n","sorted_hate_values = [dic_nb_hate_word[i] for i in sorted_hate]\n"]},{"cell_type":"code","source":["#Création d'une class pour réaliser notre bubble chart\n","\n","class BubbleChart:\n","    def __init__(self, area, bubble_spacing=0):\n","        \"\"\"\n","        Setup for bubble collapse.\n","\n","        Parameters\n","        ----------\n","        area : array-like\n","            Area of the bubbles.\n","        bubble_spacing : float, default: 0\n","            Minimal spacing between bubbles after collapsing.\n","\n","        Notes\n","        -----\n","        If \"area\" is sorted, the results might look weird.\n","        \"\"\"\n","        area = np.asarray(area)\n","        r = np.sqrt(area / np.pi)\n","\n","        self.bubble_spacing = bubble_spacing\n","        self.bubbles = np.ones((len(area), 4))\n","        self.bubbles[:, 2] = r\n","        self.bubbles[:, 3] = area\n","        self.maxstep = 2 * self.bubbles[:, 2].max() + self.bubble_spacing\n","        self.step_dist = self.maxstep / 2\n","\n","        # calculate initial grid layout for bubbles\n","        length = np.ceil(np.sqrt(len(self.bubbles)))\n","        grid = np.arange(length) * self.maxstep\n","        gx, gy = np.meshgrid(grid, grid)\n","        self.bubbles[:, 0] = gx.flatten()[:len(self.bubbles)]\n","        self.bubbles[:, 1] = gy.flatten()[:len(self.bubbles)]\n","\n","        self.com = self.center_of_mass()\n","\n","    def center_of_mass(self):\n","        return np.average(\n","            self.bubbles[:, :2], axis=0, weights=self.bubbles[:, 3]\n","        )\n","\n","    def center_distance(self, bubble, bubbles):\n","        return np.hypot(bubble[0] - bubbles[:, 0],\n","                        bubble[1] - bubbles[:, 1])\n","\n","    def outline_distance(self, bubble, bubbles):\n","        center_distance = self.center_distance(bubble, bubbles)\n","        return center_distance - bubble[2] - \\\n","            bubbles[:, 2] - self.bubble_spacing\n","\n","    def check_collisions(self, bubble, bubbles):\n","        distance = self.outline_distance(bubble, bubbles)\n","        return len(distance[distance < 0])\n","\n","    def collides_with(self, bubble, bubbles):\n","        distance = self.outline_distance(bubble, bubbles)\n","        idx_min = np.argmin(distance)\n","        return idx_min if type(idx_min) == np.ndarray else [idx_min]\n","\n","    def collapse(self, n_iterations=50):\n","        \"\"\"\n","        Move bubbles to the center of mass.\n","\n","        Parameters\n","        ----------\n","        n_iterations : int, default: 50\n","            Number of moves to perform.\n","        \"\"\"\n","        for _i in range(n_iterations):\n","            moves = 0\n","            for i in range(len(self.bubbles)):\n","                rest_bub = np.delete(self.bubbles, i, 0)\n","                # try to move directly towards the center of mass\n","                # direction vector from bubble to the center of mass\n","                dir_vec = self.com - self.bubbles[i, :2]\n","\n","                # shorten direction vector to have length of 1\n","                dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n","\n","                # calculate new bubble position\n","                new_point = self.bubbles[i, :2] + dir_vec * self.step_dist\n","                new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n","\n","                # check whether new bubble collides with other bubbles\n","                if not self.check_collisions(new_bubble, rest_bub):\n","                    self.bubbles[i, :] = new_bubble\n","                    self.com = self.center_of_mass()\n","                    moves += 1\n","                else:\n","                    # try to move around a bubble that you collide with\n","                    # find colliding bubble\n","                    for colliding in self.collides_with(new_bubble, rest_bub):\n","                        # calculate direction vector\n","                        dir_vec = rest_bub[colliding, :2] - self.bubbles[i, :2]\n","                        dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n","                        # calculate orthogonal vector\n","                        orth = np.array([dir_vec[1], -dir_vec[0]])\n","                        # test which direction to go\n","                        new_point1 = (self.bubbles[i, :2] + orth *\n","                                      self.step_dist)\n","                        new_point2 = (self.bubbles[i, :2] - orth *\n","                                      self.step_dist)\n","                        dist1 = self.center_distance(\n","                            self.com, np.array([new_point1]))\n","                        dist2 = self.center_distance(\n","                            self.com, np.array([new_point2]))\n","                        new_point = new_point1 if dist1 < dist2 else new_point2\n","                        new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n","                        if not self.check_collisions(new_bubble, rest_bub):\n","                            self.bubbles[i, :] = new_bubble\n","                            self.com = self.center_of_mass()\n","\n","            if moves / len(self.bubbles) < 0.1:\n","                self.step_dist = self.step_dist / 2\n","\n","    def plot(self, ax, labels, colors):\n","        \"\"\"\n","        Draw the bubble plot.\n","\n","        Parameters\n","        ----------\n","        ax : matplotlib.axes.Axes\n","        labels : list\n","            Labels of the bubbles.\n","        colors : list\n","            Colors of the bubbles.\n","        \"\"\"\n","        for i in range(len(self.bubbles)):\n","            circ = plt.Circle(\n","                self.bubbles[i, :2], self.bubbles[i, 2], color=colors[i])\n","            ax.add_patch(circ)\n","            ax.text(*self.bubbles[i, :2], labels[i],\n","                    horizontalalignment='center', verticalalignment='center')"],"metadata":{"id":"Y8KiGr0ALUTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8DVVKJAQBAEO"},"source":["## Visualisation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB-LtdY1BAEQ"},"outputs":[],"source":["#max(x_nhl)\n","\n","plt.plot(x_nhl,y_nhl, 'ro')\n","plt.axis([0,4000,0,1])\n","plt.title(\"Représentation de la tendance des sites à parler d'amour ou de haine\")\n","plt.xlabel(\"Rapport du nombre de mot amour/haine sur nombre de mots total\")\n","plt.ylabel(\"Score\")\n","plt.show()"]},{"cell_type":"markdown","source":["*   score = 0.5: amour et haine en quantité égale\n","*   score < 0.5 : prédominance de haine\n","*   score > 0.5: prédominance d'amour\n","\n"],"metadata":{"id":"Lw-kOLTYzezo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NozH4HmpBAEW"},"outputs":[],"source":["plt.xticks(rotation='vertical')\n","plt.bar(sorted_love, sorted_love_values, width=0.8, bottom=None, align='center')\n","plt.bar(sorted_hate, sorted_hate_values, width=0.8, bottom=None, align='center', color=\"red\")\n","plt.ylabel(\"Nombre d'occurence\")\n","plt.title(\"Les mots les plus représentés des champs lexicaux\")\n","plt.show()"]},{"cell_type":"code","source":["bubble_chart = BubbleChart(area=top_occurence_bubble['occurence'],\n","                           bubble_spacing=0.1)\n","\n","bubble_chart.collapse()\n","\n","fig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"))\n","bubble_chart.plot(\n","    ax, top_occurence_bubble['website'], top_occurence_bubble['color'])\n","ax.axis(\"off\")\n","ax.relim()\n","ax.autoscale_view()\n","ax.set_title('Top occurence website')\n","\n","plt.show()\n","plt.savefig(\"test.svg\")"],"metadata":{"id":"9vyD5Ps7K8Q_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ce graphe représente les sites les plus évocateurs des champs lexicaux amour/haine (plus grosses bulles) ainsi que leur penchant pour l'un des deux champs lexicaux (de bleu = haine à rose = amour en passant par gris = pas prédominance).\n"],"metadata":{"id":"Sfj1IMbZU0PG"}},{"cell_type":"markdown","source":["## Conclusion"],"metadata":{"id":"5z-Lu88mvunh"}},{"cell_type":"markdown","source":["Finally, we can see that websites talking about French literature are more likely to use a lexicon close to love than to hate, and that sites using a hateful lexical field are still less numerous than sites talking about love"],"metadata":{"id":"cV-XxmXSvpAa"}}],"metadata":{"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[],"collapsed_sections":["akgGkXg3wu4l","fSCs4zTAw8Ba","rsb537sMxEB2","vyeSajYBxWK2","TH9hav8jxlWO","e3vq6apSx0Ku"]}},"nbformat":4,"nbformat_minor":0}